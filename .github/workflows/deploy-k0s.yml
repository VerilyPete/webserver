name: Deploy K0s Kubernetes Cluster

on:
  workflow_dispatch:
    inputs:
      deploy_type:
        description: "Deployment type"
        required: true
        default: "fresh_deploy"
        type: choice
        options:
          - "fresh_deploy"
          - "update_app"
          - "update_cluster"

env:
  OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
  OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
  OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
  OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
  OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}
  TAILSCALE_API_KEY: ${{ secrets.TAILSCALE_API_KEY }}

jobs:
  deploy-k0s-cluster:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.deploy_type == 'fresh_deploy'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Verify required files exist
        run: |
          echo "Checking required files..."
          ls -la ./k0s/cloud-init/
          echo "Controller cloud-init file:"
          cat ./k0s/cloud-init/k0s-controller-cloud-init.yml
          echo "Worker cloud-init file:"
          cat ./k0s/cloud-init/k0s-worker-cloud-init.yml

      - name: Check for existing K8s instances
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: check_existing
        continue-on-error: true
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
          query: 'data[?contains(\"display-name\", `k8s-`)].{id: id, name: \"display-name\"}'
          silent: false

      - name: Debug check_existing output
        run: |
          echo "Raw output: '${{ steps.check_existing.outputs.output }}'"
          echo "Output type check:"
          if [ "${{ steps.check_existing.outputs.output }}" = "[]" ]; then
            echo "  - Matches '[]'"
          else
            echo "  - Does NOT match '[]'"
          fi
          if [ "${{ steps.check_existing.outputs.output }}" = "" ]; then
            echo "  - Is empty string"
          else
            echo "  - Is NOT empty string"
          fi

      - name: Clean up existing K8s instances and Tailscale entries
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        run: |
          echo "Found existing K8s instances, cleaning up..."

          # Remove from Tailscale first
            for hostname in k8s-controller k8s-worker-1 k8s-worker-2; do
              echo "Removing $hostname from Tailscale..."

              DEVICE_RESPONSE=$(curl -s -H "Authorization: Bearer $TAILSCALE_API_KEY" \
                "https://api.tailscale.com/api/v2/tailnet/-/devices")

              nodeId=$(echo "$DEVICE_RESPONSE" | \
                jq -r --arg hostname "$hostname" '.devices[] | select(.hostname == $hostname) | .nodeId // empty')

              if [ -n "$nodeId" ] && [ "$nodeId" != "null" ]; then
                curl -s -X DELETE \
                  -H "Authorization: Bearer $TAILSCALE_API_KEY" \
                  "https://api.tailscale.com/api/v2/device/$nodeId"
                echo "âœ… Removed $hostname from Tailscale"
              fi
                           done

           echo "Tailscale cleanup complete"

      - name: Find controller instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_controller_id
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-controller`)].id | [0]'
          silent: false

      - name: Clean controller instance ID
        if: steps.find_controller_id.outputs.output != '' && steps.find_controller_id.outputs.output != 'null' && steps.find_controller_id.outputs.output != '[]'
        id: clean_controller_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_controller_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned instance ID: $INSTANCE_ID"

      - name: Terminate controller instance
        if: steps.clean_controller_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_controller_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Find worker-1 instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker1_id
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-worker-1`)].id | [0]'
          silent: false

      - name: Clean worker-1 instance ID
        if: steps.find_worker1_id.outputs.output != '' && steps.find_worker1_id.outputs.output != 'null' && steps.find_worker1_id.outputs.output != '[]'
        id: clean_worker1_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_worker1_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned worker-1 instance ID: $INSTANCE_ID"

      - name: Terminate worker-1 instance
        if: steps.clean_worker1_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_worker1_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Find worker-2 instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker2_id
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-worker-2`)].id | [0]'
          silent: false

      - name: Clean worker-2 instance ID
        if: steps.find_worker2_id.outputs.output != '' && steps.find_worker2_id.outputs.output != 'null' && steps.find_worker2_id.outputs.output != '[]'
        id: clean_worker2_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_worker2_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned worker-2 instance ID: $INSTANCE_ID"

      - name: Terminate worker-2 instance
        if: steps.clean_worker2_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_worker2_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Find existing worker-1 data volume
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker1_volume
        with:
          command: >-
            bv volume list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --display-name "k8s-worker-1-data"
            --lifecycle-state AVAILABLE
            --raw-output
          query: 'data[0].id'
          silent: false
        continue-on-error: true

      - name: Clean existing worker-1 volume ID
        if: steps.find_worker1_volume.outputs.output != '' && steps.find_worker1_volume.outputs.output != 'null'
        id: clean_existing_volume_id
        run: |
          VOLUME_ID=$(echo "${{ steps.find_worker1_volume.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "volume_id=$VOLUME_ID" >> $GITHUB_OUTPUT
          echo "Cleaned existing volume ID: $VOLUME_ID"

      - name: Find volume attachments for existing worker-1 volume
        if: steps.clean_existing_volume_id.outputs.volume_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_volume_attachments
        with:
          command: >-
            compute volume-attachment list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --volume-id "${{ steps.clean_existing_volume_id.outputs.volume_id }}"
          silent: false
        continue-on-error: true

      - name: Detach existing worker-1 volume attachments
        if: steps.find_volume_attachments.outputs.output != '' && steps.find_volume_attachments.outputs.output != 'null' && steps.find_volume_attachments.outputs.output != '[]'
        run: |
          echo "Found volume attachments, detaching..."
          ATTACHMENTS='${{ steps.find_volume_attachments.outputs.output }}'
          
          # Parse JSON and extract attachment IDs
          ATTACHMENT_JSON=$(echo "$ATTACHMENTS" | sed 's/^"//; s/"$//; s/\\"/"/g')
          ATTACHMENT_IDS=$(echo "$ATTACHMENT_JSON" | jq -r '.data[]?.id // empty')
          
          if [ -n "$ATTACHMENT_IDS" ]; then
            echo "Detaching volume attachments..."
            echo "$ATTACHMENT_IDS" | while read -r attachment_id; do
              if [ -n "$attachment_id" ] && [ "$attachment_id" != "null" ]; then
                echo "Detaching attachment: $attachment_id"
                oci compute volume-attachment detach --volume-attachment-id "$attachment_id" --force --wait-for-state DETACHED || echo "Failed to detach $attachment_id, continuing..."
              fi
            done
            echo "Volume detachment completed"
            sleep 10  # Wait for detachment to complete
          else
            echo "No attachments found to detach"
          fi

      - name: Delete existing worker-1 data volume
        if: steps.clean_existing_volume_id.outputs.volume_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            bv volume delete
            --volume-id "${{ steps.clean_existing_volume_id.outputs.volume_id }}"
            --force
          silent: false

      - name: Create K0s Controller Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_controller
        with:
          command: >-
            --debug compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_SUBNET_ID }}"
            --user-data-file ./k0s/cloud-init/k0s-controller-cloud-init.yml
            --display-name "k8s-controller"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-controller\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Create block volume for worker-1
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_volume
        with:
          command: >-
            bv volume create
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --display-name "k8s-worker-1-data"
            --size-in-gbs 50
            --wait-for-state AVAILABLE
          silent: false

      - name: Create K8s Worker-1 Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_worker1
        with:
          command: >-
            compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_SUBNET_ID }}"
            --user-data-file ./k0s/cloud-init/k0s-worker-cloud-init.yml
            --display-name "k8s-worker-1"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-worker-1\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Parse worker-1 and volume IDs for attachment
        id: parse_attachment_ids
        run: |
          # Parse JSON output to extract IDs
          WORKER_OUTPUT='${{ steps.create_worker1.outputs.output }}'
          VOLUME_OUTPUT='${{ steps.create_volume.outputs.output }}'
          
          # Clean and parse JSON (remove outer quotes and unescape)
          WORKER_JSON=$(echo "$WORKER_OUTPUT" | sed 's/^"//; s/"$//; s/\\"/"/g')
          VOLUME_JSON=$(echo "$VOLUME_OUTPUT" | sed 's/^"//; s/"$//; s/\\"/"/g')
          
          # Extract IDs using jq
          WORKER_ID=$(echo "$WORKER_JSON" | jq -r '.data.id')
          VOLUME_ID=$(echo "$VOLUME_JSON" | jq -r '.data.id')
          
          echo "worker_id=$WORKER_ID" >> $GITHUB_OUTPUT
          echo "volume_id=$VOLUME_ID" >> $GITHUB_OUTPUT
          echo "Parsed worker ID: $WORKER_ID"
          echo "Parsed volume ID: $VOLUME_ID"

      - name: Attach block volume to worker-1
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute volume-attachment attach
            --instance-id "${{ steps.parse_attachment_ids.outputs.worker_id }}"
            --type paravirtualized
            --volume-id "${{ steps.parse_attachment_ids.outputs.volume_id }}"
            --wait-for-state ATTACHED
          silent: false

      - name: Confirm worker-1 and volume
        run: |
          echo "Created block volume: ${{ steps.create_volume.outputs.output }}"
          echo "Created worker-1: ${{ steps.create_worker1.outputs.output }}"

      - name: Create K8s Worker-2 Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_worker2
        with:
          command: >-
            compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_SUBNET_ID }}"
            --user-data-file ./k0s/cloud-init/k0s-worker-cloud-init.yml
            --display-name "k8s-worker-2"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-worker-2\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale for GitHub Actions
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Wait for all nodes to be accessible
        timeout-minutes: 15
        run: |
          echo "Waiting for all nodes to be accessible via Tailscale..."
          
          echo "Current Tailscale status:"
          tailscale status
          echo ""

          for node in k8s-controller k8s-worker-1 k8s-worker-2; do
            echo "Checking $node..."
            MAX_ATTEMPTS=45  # Increased attempts since cloud-init can take time
            for i in $(seq 1 $MAX_ATTEMPTS); do
              # First check if we can reach the node at all
              if ! ping -c 1 -W 2 $node >/dev/null 2>&1; then
                echo "  Attempt $i/$MAX_ATTEMPTS - $node not reachable via network, waiting 10s..."
                sleep 10
                continue
              fi
              
              # Try SSH with more detailed error reporting
              SSH_OUTPUT=$(timeout 10 ssh -v -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$node "echo 'âœ… $node is ready'" 2>&1)
              SSH_EXIT_CODE=$?
              
              if [ $SSH_EXIT_CODE -eq 0 ]; then
                echo "âœ… $node is accessible"
                break
              fi
              
              if [ $i -eq $MAX_ATTEMPTS ]; then
                echo "âŒ Failed to connect to $node after $MAX_ATTEMPTS attempts"
                echo "ðŸ” Final SSH attempt output:"
                echo "$SSH_OUTPUT"
                echo ""
                echo "ðŸ” Network connectivity:"
                ping -c 3 $node || echo "Ping failed"
                echo ""
                echo "ðŸ” Tailscale status:"
                tailscale status | grep $node || echo "$node not found in tailscale status"
                echo ""
                echo "ðŸ” Checking if SSH port is open:"
                timeout 5 nc -zv $node 22 || echo "Port 22 not accessible"
                echo ""
                echo "ðŸ” Checking SELinux and system status on $node:"
                # Try to get system info even if SSH fails
                timeout 10 ssh -o ConnectTimeout=3 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$node "
                  echo 'System info:';
                  uptime;
                  echo 'SELinux status:';
                  getenforce 2>/dev/null || echo 'SELinux not available';
                  echo 'SSH daemon status:';
                  systemctl is-active sshd || echo 'sshd status unknown';
                  echo 'Tailscale status:';
                  tailscale status 2>/dev/null || echo 'tailscale status failed';
                  echo 'Cloud-init status:';
                  cloud-init status 2>/dev/null || echo 'cloud-init status unknown';
                " 2>/dev/null || echo "Could not retrieve system status from $node"
                exit 1
              fi
              
              # Show some progress info every 5 attempts
              if [ $((i % 5)) -eq 0 ]; then
                echo "  Attempt $i/$MAX_ATTEMPTS - SSH failed, checking port 22..."
                timeout 3 nc -zv $node 22 2>/dev/null && echo "    Port 22 is open" || echo "    Port 22 not accessible yet"
              fi
              
              echo "  Attempt $i/$MAX_ATTEMPTS - waiting 10s..."
              sleep 10
            done
          done

          echo "âœ… All nodes are accessible"

      - name: Refresh Tailscale connection and DNS
        run: |
          echo "Refreshing Tailscale connection now that all nodes are online..."
          
          # Show current Tailscale status
          echo "Current Tailscale peers:"
          tailscale status
          echo ""
          
          # Force refresh of Tailscale DNS and peer list
          echo "Refreshing DNS cache..."
          tailscale status --json | jq '.Self.HostName' || echo "No hostname in status"
          
          # Wait a moment for DNS to propagate
          sleep 10
          
          echo "Testing DNS resolution for k8s nodes..."
          for hostname in k8s-controller k8s-worker-1 k8s-worker-2; do
            echo "Resolving $hostname..."
            if nslookup $hostname; then
              echo "âœ… $hostname resolved successfully"
            else
              echo "âš ï¸ Failed to resolve $hostname, but continuing..."
            fi
            echo "---"
          done

      - name: Verify K0s installation on all nodes
        timeout-minutes: 5
        run: |
          echo "Verifying K0s installation..."

          # Check controller with timeout and error handling
          echo "Checking controller..."
          if timeout 60 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 opc@k8s-controller "
            echo 'Controller status:'
            sudo systemctl status k0scontroller --no-pager --lines=10 || echo 'k0scontroller service not found'
            echo '---'
            echo 'k0s status:'
            sudo /usr/local/bin/k0s status || echo 'k0s status command failed'
            echo '---'
            echo 'k0s version:'
            /usr/local/bin/k0s version || echo 'k0s binary not found'
          "; then
            echo "âœ… Controller check completed"
          else
            echo "âŒ Controller check failed or timed out"
            echo "Attempting basic connectivity test..."
            timeout 10 ssh -o StrictHostKeyChecking=no opc@k8s-controller "echo 'Basic SSH works'; uptime" || echo "Basic SSH failed"
          fi

          # Check workers with timeout
          for worker in k8s-worker-1 k8s-worker-2; do
            echo "Checking $worker..."
            if timeout 30 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 opc@$worker "
              echo '$worker k0s binary:'
              /usr/local/bin/k0s version || echo 'k0s binary not found on $worker'
              echo 'System status:'
              uptime
            "; then
              echo "âœ… $worker check completed"
            else
              echo "âŒ $worker check failed or timed out"
            fi
          done

      - name: Join worker nodes to cluster
        run: |
          echo "=== Joining Worker Nodes to K0s Cluster ==="

          # Get the join token from controller
          echo "Retrieving join token from controller..."
          JOIN_TOKEN=$(ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo cat /tmp/worker-token.txt")

          if [ -z "$JOIN_TOKEN" ]; then
            echo "ERROR: Failed to get join token from controller"
            exit 1
          fi

          # Function to join a worker
          join_worker() {
            local worker_name=$1
            echo "Joining $worker_name to cluster..."

            # Copy token to worker
            echo "$JOIN_TOKEN" | ssh -o StrictHostKeyChecking=no opc@$worker_name "sudo tee /tmp/join-token.txt > /dev/null"

            # Install and start k0s worker with the token
            ssh -o StrictHostKeyChecking=no opc@$worker_name "
              sudo /usr/local/bin/k0s install worker --token-file /tmp/join-token.txt
              sudo systemctl daemon-reload
              sudo systemctl enable k0sworker
              sudo systemctl start k0sworker

              # Clean up token file
              sudo rm -f /tmp/join-token.txt
            "

            echo "âœ… $worker_name joined successfully"
          }

          # Join both workers
          join_worker "k8s-worker-1"
          join_worker "k8s-worker-2"

          # Wait for nodes to appear in cluster
          echo "Waiting for nodes to be ready in cluster..."
          sleep 30

          # Verify all nodes are present
          echo "Current cluster nodes:"
          ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo /usr/local/bin/k0s kubectl get nodes -o wide"

      - name: Label nodes and setup storage
        run: |
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            echo "Labeling nodes..."

            # Wait for nodes to be ready
            sudo /usr/local/bin/k0s kubectl wait --for=condition=Ready node/k8s-worker-1 --timeout=120s
            sudo /usr/local/bin/k0s kubectl wait --for=condition=Ready node/k8s-worker-2 --timeout=120s

            # Label worker nodes
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-1 node-role.kubernetes.io/worker=true --overwrite
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-2 node-role.kubernetes.io/worker=true --overwrite

            # Add storage label to worker-1
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-1 storage=local --overwrite

            echo "Creating PersistentVolumes..."
            sudo /usr/local/bin/k0s kubectl apply -f - <<'PV'
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: prometheus-pv
            spec:
              capacity:
                storage: 8Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: local-storage
              local:
                path: /mnt/data/k8s-pv-prometheus
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - k8s-worker-1
            ---
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: grafana-pv
            spec:
              capacity:
                storage: 2Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: local-storage
              local:
                path: /mnt/data/k8s-pv-grafana
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - k8s-worker-1
            PV

            echo "âœ… Nodes labeled and storage configured"
            sudo /usr/local/bin/k0s kubectl get nodes --show-labels
            sudo /usr/local/bin/k0s kubectl get pv
          EOF

      - name: Deploy Kubernetes manifests
        run: |
          echo "Deploying Kubernetes manifests..."

          # Copy manifests to controller
          scp -r -o StrictHostKeyChecking=no ./k0s/manifests opc@k8s-controller:/tmp/

          # Deploy on controller
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            cd /tmp/manifests

            # Update Cloudflare secret with actual values
            cat > 02-cloudflare-secret.yaml << SECRET
            apiVersion: v1
            kind: Secret
            metadata:
              name: cloudflare-credentials
              namespace: cloudflare-tunnel
            type: Opaque
            stringData:
              CLOUDFLARE_API_TOKEN: "${{ secrets.CLOUDFLARE_STAGING_API_TOKEN }}"
              CLOUDFLARE_ACCOUNT_ID: "${{ secrets.CLOUDFLARE_ACCOUNT_ID }}"
            SECRET

            # Apply manifests in order
            echo "Creating namespaces..."
            sudo /usr/local/bin/k0s kubectl apply -f 01-namespace.yaml

            echo "Creating Cloudflare secret..."
            sudo /usr/local/bin/k0s kubectl apply -f 02-cloudflare-secret.yaml

            echo "Installing Cloudflare controller CRDs..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/01-crds.yaml

            echo "Setting up RBAC..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/02-rbac.yaml

            echo "Deploying Cloudflare controller..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/03-deployment.yaml

            echo "Waiting for controller to be ready..."
            sudo /usr/local/bin/k0s kubectl wait --for=condition=available --timeout=300s \
              deployment/cloudflare-tunnel-controller -n cloudflare-tunnel || true

            echo "Deploying web application..."
            sudo /usr/local/bin/k0s kubectl apply -f 04-webserver/

            echo "Waiting for web deployment..."
            sudo /usr/local/bin/k0s kubectl wait --for=condition=available --timeout=300s \
              deployment/webserver -n webserver

            echo "âœ… All manifests deployed"
          EOF

      - name: Verify deployment
        run: |
          echo "=== Deployment Verification ==="
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            echo "ðŸ“Š Cluster Status:"
            echo "=================="
            sudo /usr/local/bin/k0s kubectl get nodes -o wide

            echo -e "\nðŸ“¦ All Pods:"
            echo "============"
            sudo /usr/local/bin/k0s kubectl get pods -A -o wide

            echo -e "\nðŸŒ Services:"
            echo "============"
            sudo /usr/local/bin/k0s kubectl get svc -A

            echo -e "\nðŸšª Ingress:"
            echo "==========="
            sudo /usr/local/bin/k0s kubectl get ingress -A

            echo -e "\nðŸŒ Web Application Pods:"
            echo "========================"
            sudo /usr/local/bin/k0s kubectl get pods -n webserver -o wide

            echo -e "\nðŸ“ Cloudflare Controller Status:"
            echo "================================"
            sudo /usr/local/bin/k0s kubectl get deployment -n cloudflare-tunnel

            # Get one pod's logs to verify it's running
            POD=$(sudo /usr/local/bin/k0s kubectl get pods -n cloudflare-tunnel -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ -n "$POD" ]; then
              echo -e "\nController logs (last 20 lines):"
              sudo /usr/local/bin/k0s kubectl logs -n cloudflare-tunnel $POD --tail=20 || echo "Logs not yet available"
            fi
          EOF

      - name: Test internal connectivity
        run: |
          echo "Testing web service internally..."
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            # Create a test pod and curl the service
            sudo /usr/local/bin/k0s kubectl run test-curl \
              --image=curlimages/curl \
              --rm -it \
              --restart=Never \
              -n webserver \
              -- curl -s -o /dev/null -w "HTTP Status: %{http_code}\n" http://webserver-service

            # Show service endpoints
            echo -e "\nService endpoints:"
            sudo /usr/local/bin/k0s kubectl get endpoints -n webserver
          EOF

      - name: Display summary
        run: |
          echo "========================================="
          echo "âœ… K0s Kubernetes Cluster Deployed!"
          echo "========================================="
          echo ""
          echo "ðŸ–¥ï¸  Cluster Access:"
          echo "  SSH: ssh opc@k8s-controller"
          echo "  Kubectl: sudo /usr/local/bin/k0s kubectl get pods -A"
          echo ""
          echo "ðŸŒ Application:"
          echo "  Domain: mclaurinquist.com"
          echo "  Replicas: 2 (distributed across workers)"
          echo ""
          echo "ðŸ“Š Resource Usage:"
          echo "  Controller: 1 OCPU, 6GB RAM (k0s control plane)"
          echo "  Worker-1: 1 OCPU, 6GB RAM + 20GB storage"
          echo "  Worker-2: 1 OCPU, 6GB RAM"
          echo "  Total: 3/4 OCPUs, 18/24GB RAM used"
          echo ""
          echo "ðŸ” Useful Commands:"
          echo "  Watch pods: watch 'sudo /usr/local/bin/k0s kubectl get pods -A'"
          echo "  App logs: sudo /usr/local/bin/k0s kubectl logs -f -n webserver deployment/webserver"
          echo "  Node resources: sudo /usr/local/bin/k0s kubectl top nodes"
          echo ""
          echo "ðŸ“ˆ Next Steps:"
          echo "  1. Verify website at https://mclaurinquist.com"
          echo "  2. Test rolling updates with 'update_app' workflow"
          echo "  3. Add monitoring stack when ready"
          echo "========================================="

  update-app:
    runs-on: ubuntu-latest
    if: github.event.inputs.deploy_type == 'update_app'
    timeout-minutes: 10

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Update web application
        run: |
          echo "ðŸ”„ Updating web application to latest image..."

          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            # Get current deployment status
            echo "Current deployment:"
            sudo /usr/local/bin/k0s kubectl get deployment/webserver -n webserver

            # Update image (triggers rolling update)
            sudo /usr/local/bin/k0s kubectl set image deployment/webserver \
              nginx=ghcr.io/verilypete/webserver:latest \
              -n webserver

            # Watch the rollout
            echo -e "\nðŸ“Š Rolling update in progress..."
            sudo /usr/local/bin/k0s kubectl rollout status deployment/webserver -n webserver

            # Show new pods
            echo -e "\nâœ… Updated pods:"
            sudo /usr/local/bin/k0s kubectl get pods -n webserver -o wide

            # Verify all replicas are running
            READY=$(sudo /usr/local/bin/k0s kubectl get deployment webserver -n webserver -o jsonpath='{.status.readyReplicas}')
            DESIRED=$(sudo /usr/local/bin/k0s kubectl get deployment webserver -n webserver -o jsonpath='{.spec.replicas}')

            if [ "$READY" = "$DESIRED" ]; then
              echo "âœ… All $READY replicas are running"
            else
              echo "âš ï¸  Only $READY of $DESIRED replicas are ready"
              exit 1
            fi
          EOF

      - name: Purge Cloudflare cache
        run: |
          echo "ðŸ”„ Purging Cloudflare cache..."

          response=$(curl -s -X POST \
            "https://api.cloudflare.com/client/v4/zones/${{ secrets.CLOUDFLARE_STAGING_ZONE_ID }}/purge_cache" \
            -H "Authorization: Bearer ${{ secrets.CLOUDFLARE_STAGING_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            --data '{"purge_everything":true}')

          if echo "$response" | grep -q '"success":true'; then
            echo "âœ… Cache purged successfully"
          else
            echo "âš ï¸  Cache purge may have failed: $response"
          fi

          echo ""
          echo "========================================="
          echo "âœ… Application Update Complete!"
          echo "========================================="
          echo "Check your site at https://mclaurinquist.com"
          echo "========================================="
