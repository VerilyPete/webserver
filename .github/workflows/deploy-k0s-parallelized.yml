name: Deploy K0s Kubernetes Cluster (Parallelized)

on:
  workflow_dispatch:
    inputs:
      deploy_type:
        description: "Deployment type"
        required: true
        default: "fresh_deploy"
        type: choice
        options:
          - "fresh_deploy"
          - "update_app"
          - "update_cluster"

env:
  OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
  OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
  OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
  OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
  OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}
  TAILSCALE_API_KEY: ${{ secrets.TAILSCALE_API_KEY }}

jobs:
  # ============================================================================
  # PHASE 1: CLEANUP (Sequential - Must Run First)
  # ============================================================================
  cleanup:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.deploy_type == 'fresh_deploy'
    outputs:
      cleanup_completed: ${{ steps.cleanup_status.outputs.completed }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Verify required files exist
        run: |
          echo "🔍 Checking required files..."
          ls -la ./k0s/cloud-init/
          echo "✅ Controller cloud-init file exists"
          echo "✅ Worker cloud-init file exists"

      - name: Check for existing K8s instances
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: check_existing
        continue-on-error: true
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
          query: 'data[?contains(\"display-name\", `k8s-`)].{id: id, name: \"display-name\"}'
          silent: false

      - name: Clean up existing K8s instances and Tailscale entries
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        run: |
          echo "Found existing K8s instances, cleaning up..."

          # Remove from Tailscale first
            for hostname in k8s-controller k8s-worker-1 k8s-worker-2; do
              echo "Removing $hostname from Tailscale..."

              DEVICE_RESPONSE=$(curl -s -H "Authorization: Bearer $TAILSCALE_API_KEY" \
                "https://api.tailscale.com/api/v2/tailnet/-/devices")

              nodeId=$(echo "$DEVICE_RESPONSE" | \
                jq -r --arg hostname "$hostname" '.devices[] | select(.hostname == $hostname) | .nodeId // empty')

              if [ -n "$nodeId" ] && [ "$nodeId" != "null" ]; then
                curl -s -X DELETE \
                  -H "Authorization: Bearer $TAILSCALE_API_KEY" \
                  "https://api.tailscale.com/api/v2/device/$nodeId"
                echo "✅ Removed $hostname from Tailscale"
              fi
                           done

           echo "Tailscale cleanup complete"

      - name: Find controller instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_controller_id
        continue-on-error: true
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-controller`)].id | [0]'
          silent: false

      - name: Clean controller instance ID
        if: steps.find_controller_id.outputs.output != '' && steps.find_controller_id.outputs.output != 'null' && steps.find_controller_id.outputs.output != '[]'
        id: clean_controller_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_controller_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned controller instance ID: $INSTANCE_ID"

      - name: Terminate controller instance
        if: steps.clean_controller_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_controller_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Find worker-1 instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker1_id
        continue-on-error: true
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-worker-1`)].id | [0]'
          silent: false

      - name: Clean worker-1 instance ID
        if: steps.find_worker1_id.outputs.output != '' && steps.find_worker1_id.outputs.output != 'null' && steps.find_worker1_id.outputs.output != '[]'
        id: clean_worker1_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_worker1_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned worker-1 instance ID: $INSTANCE_ID"

      - name: Terminate worker-1 instance
        if: steps.clean_worker1_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_worker1_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Find worker-2 instance ID
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker2_id
        continue-on-error: true
        with:
          command: >-
            compute instance list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --lifecycle-state RUNNING
            --raw-output
          query: 'data[?contains(\"display-name\", `k8s-worker-2`)].id | [0]'
          silent: false

      - name: Clean worker-2 instance ID
        if: steps.find_worker2_id.outputs.output != '' && steps.find_worker2_id.outputs.output != 'null' && steps.find_worker2_id.outputs.output != '[]'
        id: clean_worker2_id
        run: |
          INSTANCE_ID=$(echo "${{ steps.find_worker2_id.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Cleaned worker-2 instance ID: $INSTANCE_ID"

      - name: Terminate worker-2 instance
        if: steps.clean_worker2_id.outputs.instance_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute instance terminate
            --instance-id "${{ steps.clean_worker2_id.outputs.instance_id }}"
            --force
          silent: false

      - name: Confirm instance termination
        if: ${{ !contains(steps.check_existing.outputs.output, '[]') && steps.check_existing.outputs.output != '' }}
        run: |
          echo "✅ Instance termination commands sent"
          echo "Controller ID: ${{ steps.clean_controller_id.outputs.instance_id }}"
          echo "Worker-1 ID: ${{ steps.clean_worker1_id.outputs.instance_id }}"
          echo "Worker-2 ID: ${{ steps.clean_worker2_id.outputs.instance_id }}"

      - name: Find existing worker-1 data volume
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_worker1_volume
        with:
          command: >-
            bv volume list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --display-name "k8s-worker-1-data"
            --lifecycle-state AVAILABLE
            --raw-output
          query: "data[0].id"
          silent: false
        continue-on-error: true

      - name: Clean existing worker-1 volume ID
        if: steps.find_worker1_volume.outputs.output != '' && steps.find_worker1_volume.outputs.output != 'null'
        id: clean_existing_volume_id
        run: |
          VOLUME_ID=$(echo "${{ steps.find_worker1_volume.outputs.output }}" | tr -d '\n\r' | tr -d '"')
          echo "volume_id=$VOLUME_ID" >> $GITHUB_OUTPUT
          echo "Cleaned existing volume ID: $VOLUME_ID"

      - name: Find volume attachments for existing worker-1 volume
        if: steps.clean_existing_volume_id.outputs.volume_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: find_volume_attachments
        with:
          command: >-
            compute volume-attachment list
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --volume-id "${{ steps.clean_existing_volume_id.outputs.volume_id }}"
          silent: false
        continue-on-error: true

      - name: Detach existing worker-1 volume attachments
        if: steps.find_volume_attachments.outputs.output != '' && steps.find_volume_attachments.outputs.output != 'null' && steps.find_volume_attachments.outputs.output != '[]'
        run: |
          echo "Found volume attachments, detaching..."
          ATTACHMENTS='${{ steps.find_volume_attachments.outputs.output }}'

          # Parse JSON and extract attachment IDs
          ATTACHMENT_JSON=$(echo "$ATTACHMENTS" | sed 's/^"//; s/"$//; s/\\"/"/g')
          ATTACHMENT_IDS=$(echo "$ATTACHMENT_JSON" | jq -r '.data[]?.id // empty')

          if [ -n "$ATTACHMENT_IDS" ]; then
            echo "Detaching volume attachments..."
            echo "$ATTACHMENT_IDS" | while read -r attachment_id; do
              if [ -n "$attachment_id" ] && [ "$attachment_id" != "null" ]; then
                echo "Detaching attachment: $attachment_id"
                oci compute volume-attachment detach --volume-attachment-id "$attachment_id" --force --wait-for-state DETACHED || echo "Failed to detach $attachment_id, continuing..."
              fi
            done
            echo "Volume detachment completed"
            sleep 10  # Wait for detachment to complete
          else
            echo "No attachments found to detach"
          fi

      - name: Delete existing worker-1 data volume
        if: steps.clean_existing_volume_id.outputs.volume_id != ''
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            bv volume delete
            --volume-id "${{ steps.clean_existing_volume_id.outputs.volume_id }}"
            --force
          silent: false

      - name: Mark cleanup complete
        id: cleanup_status
        run: |
          echo "completed=true" >> $GITHUB_OUTPUT
          echo "✅ Cleanup phase completed successfully"

  # ============================================================================
  # PHASE 2: INFRASTRUCTURE CREATION (Parallel)
  # ============================================================================
  create-controller:
    runs-on: ubuntu-latest
    timeout-minutes: 12
    needs: cleanup
    if: always() && (needs.cleanup.outputs.cleanup_completed == 'true' || github.event.inputs.deploy_type != 'fresh_deploy')
    outputs:
      controller_output: ${{ steps.create_controller.outputs.output }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Create K0s Controller Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_controller
        with:
          command: >-
            compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_PRIVATE_SUBNET }}"
            --user-data-file ./k0s/cloud-init/k0s-controller-cloud-init.yml
            --display-name "k8s-controller"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-controller\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Controller creation summary
        run: |
          echo "🎛️ Controller instance created successfully"
          echo "Instance details: ${{ steps.create_controller.outputs.output }}"

  create-worker2:
    runs-on: ubuntu-latest
    timeout-minutes: 12
    needs: cleanup
    if: always() && (needs.cleanup.outputs.cleanup_completed == 'true' || github.event.inputs.deploy_type != 'fresh_deploy')
    outputs:
      worker2_output: ${{ steps.create_worker2.outputs.output }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Create K8s Worker-2 Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_worker2
        with:
          command: >-
            compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_PRIVATE_SUBNET }}"
            --user-data-file ./k0s/cloud-init/k0s-worker-cloud-init.yml
            --display-name "k8s-worker-2"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-worker-2\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Worker-2 creation summary
        run: |
          echo "👷 Worker-2 instance created successfully"
          echo "Instance details: ${{ steps.create_worker2.outputs.output }}"

  create-worker1-with-storage:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: cleanup
    if: always() && (needs.cleanup.outputs.cleanup_completed == 'true' || github.event.inputs.deploy_type != 'fresh_deploy')
    outputs:
      worker1_output: ${{ steps.create_worker1.outputs.output }}
      volume_output: ${{ steps.create_volume.outputs.output }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Create block volume for worker-1
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_volume
        with:
          command: >-
            bv volume create
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --display-name "k8s-worker-1-data"
            --size-in-gbs 50
            --wait-for-state AVAILABLE
          silent: false

      - name: Create K8s Worker-1 Instance
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        id: create_worker1
        with:
          command: >-
            compute instance launch
            --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
            --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}"
            --shape "VM.Standard.A1.Flex"
            --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}"
            --image-id "${{ secrets.OCI_CUSTOM_IMAGE }}"
            --subnet-id "${{ secrets.OCI_PRIVATE_SUBNET }}"
            --user-data-file ./k0s/cloud-init/k0s-worker-cloud-init.yml
            --display-name "k8s-worker-1"
            --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"k8s-worker-1\",\"TAILSCALE_AUTH_KEY\":\"${{ secrets.TAILSCALE_AUTH_KEY }}\"}"
            --wait-for-state RUNNING
            --max-wait-seconds 600
          silent: false

      - name: Parse worker-1 and volume IDs for attachment
        id: parse_attachment_ids
        run: |
          # Parse JSON output to extract IDs (same pattern as working deploy-k0s.yml)
          WORKER_OUTPUT='${{ steps.create_worker1.outputs.output }}'
          VOLUME_OUTPUT='${{ steps.create_volume.outputs.output }}'

          # Clean and parse JSON (remove outer quotes and unescape)
          WORKER_JSON=$(echo "$WORKER_OUTPUT" | sed 's/^"//; s/"$//; s/\\"/"/g')
          VOLUME_JSON=$(echo "$VOLUME_OUTPUT" | sed 's/^"//; s/"$//; s/\\"/"/g')

          # Extract IDs using jq
          WORKER_ID=$(echo "$WORKER_JSON" | jq -r '.data.id')
          VOLUME_ID=$(echo "$VOLUME_JSON" | jq -r '.data.id')

          echo "worker_id=$WORKER_ID" >> $GITHUB_OUTPUT
          echo "volume_id=$VOLUME_ID" >> $GITHUB_OUTPUT
          echo "Parsed worker ID: $WORKER_ID"
          echo "Parsed volume ID: $VOLUME_ID"

      - name: Attach block volume to worker-1
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        with:
          command: >-
            compute volume-attachment attach
            --instance-id "${{ steps.parse_attachment_ids.outputs.worker_id }}"
            --type paravirtualized
            --volume-id "${{ steps.parse_attachment_ids.outputs.volume_id }}"
            --wait-for-state ATTACHED
          silent: false

      - name: Confirm worker-1 and volume creation
        run: |
          echo "👷 Worker-1 instance created successfully"
          echo "💾 Block volume created and attached successfully"
          echo "Worker ID: ${{ steps.parse_attachment_ids.outputs.worker_id }}"
          echo "Volume ID: ${{ steps.parse_attachment_ids.outputs.volume_id }}"

  # ============================================================================
  # PHASE 3: INFRASTRUCTURE SETUP (Parallel where possible)
  # ============================================================================
  setup-networking:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [create-controller, create-worker1-with-storage, create-worker2]
    if: always() && needs.create-controller.result == 'success' && needs.create-worker1-with-storage.result == 'success' && needs.create-worker2.result == 'success'

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh
          echo "🔑 SSH keys configured"

      - name: Setup Tailscale for GitHub Actions
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Networking setup complete
        run: |
          echo "🌐 Networking setup completed"
          echo "✅ SSH configured"
          echo "✅ Tailscale connected"

  # ============================================================================
  # PHASE 4: WAIT FOR NODES (Sequential - needs all infrastructure)
  # ============================================================================
  wait-for-nodes:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [setup-networking]
    if: always() && needs.setup-networking.result == 'success'

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Wait for all nodes to be accessible
        run: |
          echo "⏳ Waiting for all nodes to be accessible via Tailscale..."

          echo "Current Tailscale status:"
          tailscale status
          echo ""

          for node in k8s-controller k8s-worker-1 k8s-worker-2; do
            echo "🔍 Checking $node..."
            MAX_ATTEMPTS=45
            for i in $(seq 1 $MAX_ATTEMPTS); do
              # First check if we can reach the node at all
              if ! ping -c 1 -W 2 $node >/dev/null 2>&1; then
                echo "  Attempt $i/$MAX_ATTEMPTS - $node not reachable via network, waiting 10s..."
                sleep 10
                continue
              fi
              
              # Try SSH with timeout
              if timeout 10 ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$node "echo '✅ $node is ready'" >/dev/null 2>&1; then
                echo "✅ $node is accessible"
                break
              fi
              
              if [ $i -eq $MAX_ATTEMPTS ]; then
                echo "❌ Failed to connect to $node after $MAX_ATTEMPTS attempts"
                exit 1
              fi
              
              # Show progress every 5 attempts
              if [ $((i % 5)) -eq 0 ]; then
                echo "  Attempt $i/$MAX_ATTEMPTS - SSH failed, checking port 22..."
                timeout 3 nc -zv $node 22 2>/dev/null && echo "    Port 22 is open" || echo "    Port 22 not accessible yet"
              fi
              
              echo "  Attempt $i/$MAX_ATTEMPTS - waiting 10s..."
              sleep 10
            done
          done

          echo "✅ All nodes are accessible"

      - name: Refresh Tailscale connection and DNS
        run: |
          echo "🔄 Refreshing Tailscale connection now that all nodes are online..."

          # Show current Tailscale status
          echo "Current Tailscale peers:"
          tailscale status
          echo ""

          # Force refresh of Tailscale DNS and peer list
          echo "Refreshing DNS cache..."
          tailscale status --json | jq '.Self.HostName' || echo "No hostname in status"

          # Wait a moment for DNS to propagate
          sleep 10

          echo "Testing DNS resolution for k8s nodes..."
          for hostname in k8s-controller k8s-worker-1 k8s-worker-2; do
            echo "Resolving $hostname..."
            if nslookup $hostname; then
              echo "✅ $hostname resolved successfully"
            else
              echo "⚠️ Failed to resolve $hostname, but continuing..."
            fi
            echo "---"
          done

      - name: Verify K0s installation on all nodes
        run: |
          echo "🔍 Verifying K0s installation..."

          # Check controller
          echo "Checking controller..."
          if timeout 60 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 opc@k8s-controller "
            echo 'Controller status:'
            sudo systemctl status k0scontroller --no-pager --lines=10 || echo 'k0scontroller service not found'
            echo '---'
            echo 'k0s status:'
            sudo /usr/local/bin/k0s status || echo 'k0s status command failed'
            echo '---'
            echo 'k0s version:'
            /usr/local/bin/k0s version || echo 'k0s binary not found'
          "; then
            echo "✅ Controller check completed"
          else
            echo "❌ Controller check failed or timed out"
          fi

          # Check workers
          for worker in k8s-worker-1 k8s-worker-2; do
            echo "Checking $worker..."
            if timeout 30 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 opc@$worker "
              echo '$worker k0s binary:'
              /usr/local/bin/k0s version || echo 'k0s binary not found on $worker'
              echo 'System status:'
              uptime
            "; then
              echo "✅ $worker check completed"
            else
              echo "❌ $worker check failed or timed out"
            fi
          done

  # ============================================================================
  # PHASE 5: CLUSTER FORMATION (Parallel worker joining)
  # ============================================================================
  join-worker1:
    runs-on: ubuntu-latest
    timeout-minutes: 8
    needs: wait-for-nodes

    steps:
      - name: Setup SSH and Tailscale
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Join worker-1 to cluster
        run: |
          echo "🔗 Joining worker-1 to K0s cluster..."

          # Get the join token from controller
          echo "Retrieving join token from controller..."
          JOIN_TOKEN=$(ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo cat /tmp/worker-token.txt")

          if [ -z "$JOIN_TOKEN" ]; then
            echo "ERROR: Failed to get join token from controller"
            exit 1
          fi

          echo "Installing k0s worker on k8s-worker-1..."
          if ssh -o StrictHostKeyChecking=no opc@k8s-worker-1 "
            # Copy token to worker
            echo '$JOIN_TOKEN' | sudo tee /tmp/join-token.txt > /dev/null
            
            # Install and start k0s worker
            sudo /usr/local/bin/k0s install worker --token-file /tmp/join-token.txt
            sudo systemctl daemon-reload
            sudo systemctl enable k0sworker
            sudo systemctl start k0sworker

            # Wait and check service
            sleep 5
            if systemctl is-active --quiet k0sworker; then
              echo 'k0sworker service is running'
            else
              echo 'ERROR: k0sworker service failed to start'
              systemctl status k0sworker --no-pager -l
              exit 1
            fi

            # Clean up token
            sudo rm -f /tmp/join-token.txt
          "; then
            echo "✅ Worker-1 joined successfully"
          else
            echo "❌ Failed to join worker-1 to cluster"
            exit 1
          fi

  join-worker2:
    runs-on: ubuntu-latest
    timeout-minutes: 8
    needs: wait-for-nodes

    steps:
      - name: Setup SSH and Tailscale
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Join worker-2 to cluster
        run: |
          echo "🔗 Joining worker-2 to K0s cluster..."

          # Get the join token from controller
          echo "Retrieving join token from controller..."
          JOIN_TOKEN=$(ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo cat /tmp/worker-token.txt")

          if [ -z "$JOIN_TOKEN" ]; then
            echo "ERROR: Failed to get join token from controller"
            exit 1
          fi

          echo "Installing k0s worker on k8s-worker-2..."
          if ssh -o StrictHostKeyChecking=no opc@k8s-worker-2 "
            # Copy token to worker
            echo '$JOIN_TOKEN' | sudo tee /tmp/join-token.txt > /dev/null
            
            # Install and start k0s worker
            sudo /usr/local/bin/k0s install worker --token-file /tmp/join-token.txt
            sudo systemctl daemon-reload
            sudo systemctl enable k0sworker
            sudo systemctl start k0sworker

            # Wait and check service
            sleep 5
            if systemctl is-active --quiet k0sworker; then
              echo 'k0sworker service is running'
            else
              echo 'ERROR: k0sworker service failed to start'
              systemctl status k0sworker --no-pager -l
              exit 1
            fi

            # Clean up token
            sudo rm -f /tmp/join-token.txt
          "; then
            echo "✅ Worker-2 joined successfully"
          else
            echo "❌ Failed to join worker-2 to cluster"
            exit 1
          fi

  # ============================================================================
  # PHASE 6: CLUSTER VERIFICATION AND CONFIGURATION
  # ============================================================================
  verify-cluster:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [join-worker1, join-worker2]

    steps:
      - name: Setup SSH and Tailscale
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Verify cluster formation
        run: |
          echo "🔍 Verifying cluster formation..."

          # Wait for nodes to appear in cluster
          echo "Waiting for nodes to be ready in cluster..."
          sleep 60

          # Verify all nodes are present with retries
          echo "Verifying cluster nodes..."
          for i in {1..10}; do
            echo "Attempt $i/10 - Checking cluster nodes:"
            NODE_COUNT=$(ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo /usr/local/bin/k0s kubectl get nodes --no-headers 2>/dev/null | wc -l" || echo "0")
            echo "Found $NODE_COUNT nodes in cluster"
            
            if [ "$NODE_COUNT" -ge 2 ]; then
              echo "✅ All worker nodes found in cluster (controller is controller-only)"
              ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo /usr/local/bin/k0s kubectl get nodes -o wide"
              break
            else
              echo "⏳ Only $NODE_COUNT/2 worker nodes found, waiting 15s..."
              if [ $i -eq 10 ]; then
                echo "❌ Timeout waiting for worker nodes to join"
                echo "Current nodes:"
                ssh -o StrictHostKeyChecking=no opc@k8s-controller "sudo /usr/local/bin/k0s kubectl get nodes -o wide" || echo "Failed to get nodes"
                echo "Note: Controller is controller-only and won't appear as a node"
                exit 1
              fi
              sleep 15
            fi
          done

  configure-cluster:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: verify-cluster

    steps:
      - name: Setup SSH and Tailscale
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Label nodes and setup storage
        run: |
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            echo "🏷️ Labeling nodes..."

            # Wait for nodes to be ready
            sudo /usr/local/bin/k0s kubectl wait --for=condition=Ready node/k8s-worker-1 --timeout=120s
            sudo /usr/local/bin/k0s kubectl wait --for=condition=Ready node/k8s-worker-2 --timeout=120s

            # Label worker nodes
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-1 node-role.kubernetes.io/worker=true --overwrite
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-2 node-role.kubernetes.io/worker=true --overwrite

            # Add storage label to worker-1
            sudo /usr/local/bin/k0s kubectl label node k8s-worker-1 storage=local --overwrite

            echo "💾 Creating PersistentVolumes..."
            
            # Create prometheus PV
            cat > /tmp/prometheus-pv.yaml << 'PVEND'
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: prometheus-pv
            spec:
              capacity:
                storage: 20Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: local-storage
              local:
                path: /mnt/data/k8s-pv-prometheus
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - k8s-worker-1
          PVEND
            
            # Create grafana PV
            cat > /tmp/grafana-pv.yaml << 'PVEND'
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: grafana-pv
            spec:
              capacity:
                storage: 8Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: local-storage
              local:
                path: /mnt/data/k8s-pv-grafana
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - k8s-worker-1
          PVEND
            
            # Apply the PVs
            sudo /usr/local/bin/k0s kubectl apply -f /tmp/prometheus-pv.yaml
            sudo /usr/local/bin/k0s kubectl apply -f /tmp/grafana-pv.yaml
            
            # Clean up temp files
            rm -f /tmp/prometheus-pv.yaml /tmp/grafana-pv.yaml

            echo "✅ Nodes labeled and storage configured"
            sudo /usr/local/bin/k0s kubectl get nodes --show-labels
            sudo /usr/local/bin/k0s kubectl get pv
          EOF

  # ============================================================================
  # PHASE 7: APPLICATION DEPLOYMENT
  # ============================================================================
  deploy-applications:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: configure-cluster

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup SSH and Tailscale
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Deploy Kubernetes manifests
        run: |
          echo "🚀 Deploying Kubernetes manifests..."

          # Copy manifests to controller
          scp -r -o StrictHostKeyChecking=no ./k0s/manifests opc@k8s-controller:/tmp/

          # Deploy on controller
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            cd /tmp/manifests

            # Update Cloudflare secret with actual values
            cat > 02-cloudflare-secret.yaml << 'SECRETEND'
            apiVersion: v1
            kind: Secret
            metadata:
              name: cloudflare-credentials
              namespace: cloudflare-tunnel
            type: Opaque
            stringData:
              CLOUDFLARE_API_TOKEN: "${{ secrets.CLOUDFLARE_STAGING_API_TOKEN }}"
              CLOUDFLARE_ACCOUNT_ID: "${{ secrets.CLOUDFLARE_ACCOUNT_ID }}"
          SECRETEND

            # Apply manifests in order
            echo "Creating namespaces..."
            sudo /usr/local/bin/k0s kubectl apply -f 01-namespace.yaml

            echo "Creating Cloudflare secret..."
            sudo /usr/local/bin/k0s kubectl apply -f 02-cloudflare-secret.yaml

            echo "Installing Cloudflare controller CRDs..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/01-crds.yaml

            echo "Setting up RBAC..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/02-rbac.yaml

            echo "Deploying Cloudflare controller..."
            sudo /usr/local/bin/k0s kubectl apply -f 03-cloudflare-controller/03-deployment.yaml

            echo "Waiting for controller to be ready..."
            sudo /usr/local/bin/k0s kubectl wait --for=condition=available --timeout=300s \
              deployment/cloudflare-tunnel-controller -n cloudflare-tunnel || true
            
            echo "Controller deployment status:"
            sudo /usr/local/bin/k0s kubectl get deployment -n cloudflare-tunnel -o wide
            echo "Controller pods:"
            sudo /usr/local/bin/k0s kubectl get pods -n cloudflare-tunnel -o wide
            echo "Controller events:"
            sudo /usr/local/bin/k0s kubectl get events -n cloudflare-tunnel --sort-by='.lastTimestamp'
            echo "Controller logs (if pod exists):"
            sudo /usr/local/bin/k0s kubectl logs -n cloudflare-tunnel -l app=cloudflare-tunnel-controller --tail=50 || echo "No logs available yet"

            echo "Deploying web application..."
            sudo /usr/local/bin/k0s kubectl apply -f 04-webserver/

            echo "Waiting for web deployment..."
            sudo /usr/local/bin/k0s kubectl wait --for=condition=available --timeout=300s \
              deployment/webserver -n webserver

            echo "✅ All manifests deployed"
          EOF

      - name: Verify deployment
        run: |
          echo "🔍 Deployment Verification"
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            echo "📊 Cluster Status:"
            echo "=================="
            sudo /usr/local/bin/k0s kubectl get nodes -o wide

            echo -e "\n📦 All Pods:"
            echo "============"
            sudo /usr/local/bin/k0s kubectl get pods -A -o wide

            echo -e "\n🌐 Services:"
            echo "============"
            sudo /usr/local/bin/k0s kubectl get svc -A

            echo -e "\n🚪 Ingress:"
            echo "==========="
            sudo /usr/local/bin/k0s kubectl get ingress -A

            echo -e "\n🌍 Web Application Pods:"
            echo "========================"
            sudo /usr/local/bin/k0s kubectl get pods -n webserver -o wide

            echo -e "\n📝 Cloudflare Controller Status:"
            echo "================================"
            sudo /usr/local/bin/k0s kubectl get deployment -n cloudflare-tunnel

            # Get controller logs
            POD=$(sudo /usr/local/bin/k0s kubectl get pods -n cloudflare-tunnel -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ -n "$POD" ]; then
              echo -e "\nController logs (last 20 lines):"
              sudo /usr/local/bin/k0s kubectl logs -n cloudflare-tunnel $POD --tail=20 || echo "Logs not yet available"
            fi
          EOF

      - name: Test internal connectivity
        run: |
          echo "🧪 Testing web service internally..."
          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            # Check service endpoints first
            echo "Service endpoints:"
            sudo /usr/local/bin/k0s kubectl get endpoints -n webserver
            echo ""
            
            # Test connectivity using a job instead of interactive pod
            echo "Testing web service connectivity..."
            
            # Create test job manifest
            cat > /tmp/test-connectivity-job.yaml << 'JOBEND'
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: test-connectivity
              namespace: webserver
            spec:
              ttlSecondsAfterFinished: 300
              template:
                spec:
                  restartPolicy: Never
                  containers:
                  - name: curl-test
                    image: docker.io/curlimages/curl:latest
                    command: ["curl"]
                    args: ["-s", "-o", "/dev/null", "-w", "HTTP Status: %{http_code}\n", "http://webserver-service"]
          JOBEND
            
            # Apply the job
            sudo /usr/local/bin/k0s kubectl apply -f /tmp/test-connectivity-job.yaml
            
            # Wait for job completion
            echo "Waiting for connectivity test to complete..."
            sudo /usr/local/bin/k0s kubectl wait --for=condition=complete --timeout=60s job/test-connectivity -n webserver
            
            # Get the results
            echo "Test results:"
            sudo /usr/local/bin/k0s kubectl logs job/test-connectivity -n webserver
            
            # Clean up
            sudo /usr/local/bin/k0s kubectl delete job test-connectivity -n webserver
            rm -f /tmp/test-connectivity-job.yaml
          EOF

      - name: Display deployment summary
        run: |
          echo "========================================="
          echo "🎉 K0s Kubernetes Cluster Deployed!"
          echo "========================================="
          echo ""
          echo "🖥️  Cluster Access:"
          echo "  SSH: ssh opc@k8s-controller"
          echo "  Kubectl: sudo /usr/local/bin/k0s kubectl get pods -A"
          echo ""
          echo "🌐 Application:"
          echo "  Domain: mclaurinquist.com"
          echo "  Replicas: 2 (distributed across workers)"
          echo ""
          echo "📊 Resource Usage:"
          echo "  Controller: 1 OCPU, 6GB RAM (k0s control plane)"
          echo "  Worker-1: 1 OCPU, 6GB RAM + 50GB storage"
          echo "  Worker-2: 1 OCPU, 6GB RAM"
          echo "  Total: 3/4 OCPUs, 18/24GB RAM used"
          echo ""
          echo "🔍 Useful Commands:"
          echo "  Watch pods: watch 'sudo /usr/local/bin/k0s kubectl get pods -A'"
          echo "  App logs: sudo /usr/local/bin/k0s kubectl logs -f -n webserver deployment/webserver"
          echo "  Node resources: sudo /usr/local/bin/k0s kubectl top nodes"
          echo ""
          echo "📈 Next Steps:"
          echo "  1. Verify website at https://mclaurinquist.com"
          echo "  2. Test rolling updates with 'update_app' workflow"
          echo "  3. Add monitoring stack when ready"
          echo "========================================="

  # ============================================================================
  # UPDATE APP JOB (Unchanged from original)
  # ============================================================================
  update-app:
    runs-on: ubuntu-latest
    if: github.event.inputs.deploy_type == 'update_app'
    timeout-minutes: 10

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Setup Tailscale
        uses: tailscale/github-action@v2
        with:
          authkey: ${{ secrets.PRIVATE_TAILSCALE_KEY }}
          tags: tag:private-deploy

      - name: Update web application
        run: |
          echo "🔄 Updating web application to latest image..."

          ssh -o StrictHostKeyChecking=no opc@k8s-controller << 'EOF'
            # Get current deployment status
            echo "Current deployment:"
            sudo /usr/local/bin/k0s kubectl get deployment/webserver -n webserver

            # Update image (triggers rolling update)
            sudo /usr/local/bin/k0s kubectl set image deployment/webserver \
              nginx=ghcr.io/verilypete/webserver:latest \
              -n webserver

            # Watch the rollout
            echo -e "\n📊 Rolling update in progress..."
            sudo /usr/local/bin/k0s kubectl rollout status deployment/webserver -n webserver

            # Show new pods
            echo -e "\n✅ Updated pods:"
            sudo /usr/local/bin/k0s kubectl get pods -n webserver -o wide

            # Verify all replicas are running
            READY=$(sudo /usr/local/bin/k0s kubectl get deployment webserver -n webserver -o jsonpath='{.status.readyReplicas}')
            DESIRED=$(sudo /usr/local/bin/k0s kubectl get deployment webserver -n webserver -o jsonpath='{.spec.replicas}')

            if [ "$READY" = "$DESIRED" ]; then
              echo "✅ All $READY replicas are running"
            else
              echo "⚠️  Only $READY of $DESIRED replicas are ready"
              exit 1
            fi
          EOF

      - name: Purge Cloudflare cache
        run: |
          echo "🔄 Purging Cloudflare cache..."

          response=$(curl -s -X POST \
            "https://api.cloudflare.com/client/v4/zones/${{ secrets.CLOUDFLARE_STAGING_ZONE_ID }}/purge_cache" \
            -H "Authorization: Bearer ${{ secrets.CLOUDFLARE_STAGING_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            --data '{"purge_everything":true}')

          if echo "$response" | grep -q '"success":true'; then
            echo "✅ Cache purged successfully"
          else
            echo "⚠️  Cache purge may have failed: $response"
          fi

          echo ""
          echo "========================================="
          echo "✅ Application Update Complete!"
          echo "========================================="
          echo "Check your site at https://mclaurinquist.com"
          echo "========================================="
