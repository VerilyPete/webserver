# .github/workflows/deploy-infrastructure.yml
name: Deploy Web Infrastructure

on:
  workflow_dispatch:
    inputs:
      deploy_type:
        description: 'Deployment type'
        required: true
        default: 'update'
        type: choice
        options:
          - update
          - fresh_deploy
      hostname:
        description: 'Custom hostname (optional)'
        required: false
        type: string

env:
  OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
  OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
  OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
  OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
  OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased to accommodate cloud-init delays
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate unique hostname
        id: hostname
        run: |
          if [ -n "${{ github.event.inputs.hostname }}" ]; then
            echo "hostname=${{ github.event.inputs.hostname }}" >> $GITHUB_OUTPUT
          else
            echo "hostname=web-server-$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT
          fi

      - name: Deploy new instance (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 20  # Increased timeout for instance creation
        id: create_instance
        with:
          command: 'compute instance launch --availability-domain "${{ secrets.OCI_AVAILABILITY_DOMAIN }}" --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --shape "VM.Standard.A1.Flex" --shape-config "{\"memoryInGBs\":6,\"ocpus\":1}" --image-id "${{ secrets.OCI_IMAGE_ID }}" --subnet-id "${{ secrets.OCI_SUBNET_ID }}" --user-data-file cloud-init.yml --display-name "${{ steps.hostname.outputs.hostname }}" --metadata "{\"ssh_authorized_keys\":\"${{ secrets.SSH_PUBLIC_KEY }}\",\"HOSTNAME\":\"${{ steps.hostname.outputs.hostname }}\"}" --wait-for-state RUNNING --max-wait-seconds 900'
          silent: false

      - name: Debug environment variables
        if: github.event.inputs.deploy_type == 'update'
        run: |
          echo "=== GitHub Actions Environment ==="
          echo "GITHUB_ACTOR: $GITHUB_ACTOR"
          echo "GITHUB_REPOSITORY: $GITHUB_REPOSITORY"
          
          echo "=== OCI Environment Variables (partial) ==="
          echo "OCI_CLI_USER: ${OCI_CLI_USER:0:25}..."
          echo "OCI_CLI_TENANCY: ${OCI_CLI_TENANCY:0:25}..."
          echo "OCI_CLI_REGION: $OCI_CLI_REGION"
          echo "OCI_CLI_FINGERPRINT: $OCI_CLI_FINGERPRINT"
          
          echo "=== Validating OCID Formats ==="
          if [[ $OCI_CLI_USER == ocid1.user.oc1.* ]]; then
            echo "‚úÖ User OCID format looks correct"
          else
            echo "‚ùå User OCID format looks wrong - should start with 'ocid1.user.oc1.'"
          fi
          
          if [[ $OCI_CLI_TENANCY == ocid1.tenancy.oc1.* ]]; then
            echo "‚úÖ Tenancy OCID format looks correct"  
          else
            echo "‚ùå Tenancy OCID format looks wrong - should start with 'ocid1.tenancy.oc1.'"
          fi
          
          if [[ $OCI_CLI_FINGERPRINT =~ ^[a-f0-9]{2}(:[a-f0-9]{2}){15}$ ]]; then
            echo "‚úÖ Fingerprint format looks correct"
          else
            echo "‚ùå Fingerprint format looks wrong - should be 16 colon-separated hex pairs"
          fi

      - name: Test basic OCI connectivity
        if: github.event.inputs.deploy_type == 'update'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 2  # Increased slightly
        continue-on-error: true
        id: test_connection
        with:
          command: 'iam region list'
          silent: false

      - name: Debug OCI connection result
        if: github.event.inputs.deploy_type == 'update'
        run: |
          echo "=== OCI Connection Test Result ==="
          if [ "${{ steps.test_connection.outcome }}" = "success" ]; then
            echo "‚úÖ OCI connection successful"
            echo "Regions output: ${{ steps.test_connection.outputs.output }}"
          else
            echo "‚ùå OCI connection failed"
            echo "This suggests an authentication issue with your OCI credentials"
          fi

      - name: Find existing instance (update)
        if: github.event.inputs.deploy_type == 'update'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 3  # Increased timeout
        id: find_instance
        continue-on-error: true
        with:
          command: 'compute instance list --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --lifecycle-state RUNNING'
          query: 'data[?contains(\"display-name\", `web-server`)].id | [0]'
          silent: false

      - name: Check if instance was found
        if: github.event.inputs.deploy_type == 'update'
        id: check_instance
        run: |
          INSTANCE_ID="${{ steps.find_instance.outputs.output }}"
          if [ "$INSTANCE_ID" = "null" ] || [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = '""' ]; then
            echo "No running instance found with 'web-server' in the name"
            echo "found=false" >> $GITHUB_OUTPUT
            echo "instance_id=" >> $GITHUB_OUTPUT
          else
            echo "Found instance ID: $INSTANCE_ID"
            echo "found=true" >> $GITHUB_OUTPUT
            echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
          fi

      - name: Get instance IP and update (update)
        if: github.event.inputs.deploy_type == 'update' && steps.check_instance.outputs.found == 'true'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 2
        id: get_instance_ip
        with:
          command: 'compute instance list-vnics --instance-id "${{ steps.check_instance.outputs.instance_id }}"'
          query: 'data[0].\"public-ip\"'

      - name: Setup SSH key for update
        if: github.event.inputs.deploy_type == 'update' && steps.check_instance.outputs.found == 'true'
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Update existing infrastructure
        if: github.event.inputs.deploy_type == 'update' && steps.check_instance.outputs.found == 'true'
        timeout-minutes: 12  # Increased timeout
        run: |
          PUBLIC_IP="${{ steps.get_instance_ip.outputs.output }}"
          PUBLIC_IP=$(echo $PUBLIC_IP | tr -d '"')
          
          echo "Found instance with IP: $PUBLIC_IP"
          
          # Robust SSH connectivity check
          echo "Waiting for SSH access..."
          SSH_READY=false
          for i in {1..36}; do  # 36 attempts √ó 10 seconds = 6 minutes max
            if timeout 10 ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$PUBLIC_IP "echo 'SSH Ready'" 2>/dev/null; then
              echo "‚úÖ SSH connection successful (attempt $i)"
              SSH_READY=true
              break
            fi
            echo "SSH attempt $i/36 failed, waiting 10 seconds..."
            sleep 10
          done
          
          if [ "$SSH_READY" = "false" ]; then
            echo "‚ùå SSH connection failed after 6 minutes"
            echo "Instance may be down or network issues present"
            exit 1
          fi
          
          # Update the running infrastructure using deploy.sh script
          echo "Updating infrastructure using deploy.sh script..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 opc@$PUBLIC_IP << 'EOF'
            set -e  # Exit on any error
            
            echo "Ensuring system configuration is up to date..."
            
            # Enable user lingering and setup subuid/subgid (if not already done)
            sudo loginctl enable-linger opc
            if ! grep -q "^opc:" /etc/subuid; then echo "opc:100000:65536" >> /etc/subuid; fi
            if ! grep -q "^opc:" /etc/subgid; then echo "opc:100000:65536" >> /etc/subgid; fi
            
            # Setup cron job for podman cleanup (if not already done)
            if ! crontab -l 2>/dev/null | grep -q "podman-cleanup.sh"; then
              (crontab -l 2>/dev/null; echo "0 2 * * * /usr/local/bin/podman-cleanup.sh >> /var/log/podman-cleanup.log 2>&1") | crontab -
            fi
            
            # Create cleanup script (if not exists)
            if [ ! -f "/usr/local/bin/podman-cleanup.sh" ]; then
              sudo tee /usr/local/bin/podman-cleanup.sh > /dev/null << 'CLEANUP_EOF'
              #!/bin/bash
              for user in $(getent passwd | grep -E '/home|/var/lib' | cut -d: -f1); do
                if id "$user" &>/dev/null; then
                  sudo -u "$user" podman container prune -f 2>/dev/null || true
                  sudo -u "$user" podman image prune -af --filter "until=24h" 2>/dev/null || true
                  sudo -u "$user" podman volume prune -f 2>/dev/null || true
                  sudo -u "$user" podman system prune -af 2>/dev/null || true
                fi
              done
              podman container prune -f; podman image prune -af --filter "until=24h"; podman volume prune -f; podman system prune -af
              buildah rmi --prune; journalctl --vacuum-time=7d; journalctl --vacuum-size=500M
              CLEANUP_EOF
              sudo chmod +x /usr/local/bin/podman-cleanup.sh
            fi
            
            echo "‚úÖ System configuration verified"
            
            echo "Ensuring container infrastructure is properly configured..."
            
            # Create systemd service files (if not exists)
            mkdir -p ~/.config/systemd/user
            
            cat > ~/.config/systemd/user/webserver-pod.service << 'SERVICE_EOF'
            [Unit]
            Description=Web Infrastructure Pod
            Wants=network-online.target
            After=network-online.target
            RequiresMountsFor=%t/containers
            [Service]
            Type=simple
            Restart=on-failure
            TimeoutStopSec=70
            WorkingDirectory=%h/webserver
            Environment=PODMAN_SYSTEMD_UNIT=%n
            ExecStartPre=/usr/bin/podman pod create --name webserver-pod --publish 8081:8081 --replace
            ExecStart=/usr/bin/bash -c 'cd %h/webserver && /usr/local/bin/start-web-pod.sh'
            ExecStop=/usr/bin/podman pod stop webserver-pod
            ExecStopPost=/usr/bin/podman pod rm -f webserver-pod
            [Install]
            WantedBy=default.target
            SERVICE_EOF
            
            # Create start script (if not exists)
            if [ ! -f "/usr/local/bin/start-web-pod.sh" ]; then
              sudo tee /usr/local/bin/start-web-pod.sh > /dev/null << 'START_EOF'
              #!/bin/bash
              set -e
              if [ -f ".env" ]; then
                set -a && source .env && set +a
              else
                echo "ERROR: .env file not found" && exit 1
              fi
              
              # Ensure the pod exists
              echo "Ensuring webserver-pod exists..."
              podman pod create --name webserver-pod --publish 8081:8081 --replace 2>/dev/null || true
              
              # Stop and remove existing containers to avoid conflicts
              podman stop web-pod web tailscale cloudflared 2>/dev/null || true
              podman rm web-pod web tailscale cloudflared 2>/dev/null || true
              
              # Start containers
              podman run -d --name web-pod --pod webserver-pod --restart unless-stopped k8s.gcr.io/pause:3.9
              podman run -d --name web --pod webserver-pod --restart unless-stopped ghcr.io/verilypete/webserver:latest
              if [ ! -z "$TAILSCALE_AUTH_KEY" ] && [ "$TAILSCALE_AUTH_KEY" != "tskey-auth-xxxxxxxxx" ]; then
                podman run -d --name tailscale --pod webserver-pod --privileged --restart unless-stopped \
                  --volume tailscale-data:/var/lib/tailscale:z --volume /dev/net/tun:/dev/net/tun --cap-add NET_ADMIN \
                  --env TS_AUTHKEY="$TAILSCALE_AUTH_KEY" --env TS_HOSTNAME="$HOSTNAME" --env TS_STATE_DIR=/var/lib/tailscale \
                  tailscale/tailscale:latest
              fi
              if [ ! -z "$CLOUDFLARE_TUNNEL_TOKEN" ] && [ "$CLOUDFLARE_TUNNEL_TOKEN" != "your-tunnel-token-here" ]; then
                podman run -d --name cloudflared --pod webserver-pod --restart unless-stopped \
                  --env TUNNEL_TOKEN="$CLOUDFLARE_TUNNEL_TOKEN" cloudflare/cloudflared:latest tunnel --no-autoupdate run
              fi
              
              echo "‚úÖ Web infrastructure started successfully"
              START_EOF
              sudo chmod +x /usr/local/bin/start-web-pod.sh
            fi
            
            # Setup user systemd environment
            export XDG_RUNTIME_DIR="/run/user/$(id -u)"
            sudo systemctl start user@$(id -u).service
            sleep 2
            systemctl --user daemon-reload
            systemctl --user enable webserver-pod.service
            
            echo "‚úÖ Container infrastructure verified"
            
            echo "Updating repository..."
            cd ~/webserver
            git pull origin main || {
              echo "‚ùå Git pull failed"
              exit 1
            }
            
            echo "Creating .env file..."
            cat > .env << 'ENV_EOF'
            HOSTNAME=${{ steps.hostname.outputs.hostname }}
            TAILSCALE_AUTH_KEY=${{ secrets.TAILSCALE_AUTH_KEY }}
            CLOUDFLARE_TUNNEL_TOKEN=${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
            FORMSPREE_ENDPOINT=${{ secrets.FORMSPREE_ENDPOINT }}
            APP_PORT=8081
            APP_ENV=production
            ENV_EOF
            
            chmod 600 .env
            
            echo "Running deploy.sh script..."
            ./scripts/deploy.sh || {
              echo "‚ùå Deploy script failed"
              exit 1
            }
            
            echo "‚úÖ Infrastructure update completed successfully"
          EOF

      - name: Handle no existing instance
        if: github.event.inputs.deploy_type == 'update' && steps.check_instance.outputs.found == 'false'
        run: |
          echo "‚ùå No running instance found with 'web-server' in the name"
          echo "üí° Use 'fresh_deploy' to create a new instance"
          exit 1

      - name: Debug instance creation output
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        id: parse_instance_id
        run: |
          echo "=== Raw create_instance output ==="
          echo '${{ steps.create_instance.outputs.output }}'
          echo ""
          echo "=== Extracting instance ID ==="
          # Remove outer quotes and unescape the JSON string
          CLEAN_JSON=$(echo '${{ steps.create_instance.outputs.output }}' | sed 's/^"//; s/"$//; s/\\"/"/g')
          echo "Cleaned JSON:"
          echo "$CLEAN_JSON"
          echo ""
          INSTANCE_ID=$(echo "$CLEAN_JSON" | jq -r '.data.id')
          echo "Extracted instance ID: $INSTANCE_ID"
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

      - name: Display new instance info (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 2
        continue-on-error: true
        with:
          command: 'compute instance get --instance-id "${{ steps.parse_instance_id.outputs.instance_id }}"'
          query: 'data.{id: id, name: \"display-name\", state: \"lifecycle-state\", shape: shape, region: region}'

      - name: Get new instance IP (fresh deploy)
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 3
        id: get_new_ip
        continue-on-error: true
        with:
          command: 'compute instance list-vnics --instance-id "${{ steps.parse_instance_id.outputs.instance_id }}"'
          query: 'data[0].\"public-ip\"'

      - name: Setup SSH key for fresh deploy
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          chmod 700 ~/.ssh

      - name: Wait for cloud-init and services to be ready
        if: github.event.inputs.deploy_type == 'fresh_deploy' || github.event_name == 'workflow_dispatch'
        timeout-minutes: 20
        run: |
          PUBLIC_IP=$(echo '${{ steps.get_new_ip.outputs.output }}' | tr -d '"')
          echo "Waiting for instance to be fully ready at $PUBLIC_IP..."
          
          # Function to check SSH connectivity
          check_ssh() {
            timeout 8 ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes opc@$PUBLIC_IP "echo 'SSH Ready'" >/dev/null 2>&1
          }
          
          # Step 1: Wait for SSH (max 5 minutes)
          echo "Step 1: Waiting for SSH access..."
          SSH_ATTEMPTS=30  # 30 * 10 seconds = 5 minutes
          for i in $(seq 1 $SSH_ATTEMPTS); do
            if check_ssh; then
              echo "‚úÖ SSH connection successful (attempt $i/$SSH_ATTEMPTS)"
              break
            fi
            if [ $i -eq $SSH_ATTEMPTS ]; then
              echo "‚ùå SSH connection failed after 5 minutes"
              exit 1
            fi
            echo "SSH attempt $i/$SSH_ATTEMPTS failed, waiting 10 seconds..."
            sleep 10
          done
          
          # Step 2: Wait for cloud-init with simpler, more reliable approach
          echo "Step 2: Waiting for cloud-init to complete..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=15 opc@$PUBLIC_IP << 'CLOUD_INIT_EOF'
          set -e
          
          echo "Checking cloud-init status..."
          
          # Improved cloud-init monitoring with better error handling
          echo "Waiting for cloud-init to complete..."
          
          # First, wait for cloud-init to finish (with timeout)
          if timeout 900 cloud-init status --wait; then
            echo "‚úÖ Cloud-init completed within timeout"
          else
            EXIT_CODE=$?
            echo "‚ö†Ô∏è Cloud-init wait exited with code $EXIT_CODE"
            
            if [ $EXIT_CODE -eq 124 ]; then
              echo "‚ö†Ô∏è Timeout: cloud-init exceeded 15 minutes, checking current status..."
            fi
          fi
          
          # Check current cloud-init status regardless of timeout
          echo "Current cloud-init status:"
          cloud-init status --long || true
          
          # More lenient status checking - accept 'done' or 'running' as success
          CURRENT_STATUS=$(cloud-init status | grep "status:" | awk '{print $2}' || echo "unknown")
          echo "Cloud-init status: $CURRENT_STATUS"
          
          if [ "$CURRENT_STATUS" = "done" ]; then
            echo "‚úÖ Cloud-init completed successfully (status: done)"
          elif [ "$CURRENT_STATUS" = "running" ]; then
            echo "‚ö†Ô∏è Cloud-init still running, but continuing with deployment..."
          elif [ "$CURRENT_STATUS" = "error" ]; then
            echo "‚ùå Cloud-init reported error status"
            cloud-init status --long | head -20
            exit 1
          else
            echo "‚ö†Ô∏è Cloud-init status: $CURRENT_STATUS - continuing anyway..."
          fi
          
          # Check for critical errors only (ignore warnings)
          echo "Checking for critical cloud-init errors..."
          if cloud-init status --long | grep -A 20 "errors:" | grep -v "errors: \[\]" | grep -q "ERROR\|CRITICAL\|FATAL"; then
            echo "‚ùå Cloud-init has critical errors:"
            cloud-init status --long | grep -A 10 "errors:" | grep -i "error\|critical\|fatal" | head -5
            exit 1
          else
            echo "‚úÖ No critical cloud-init errors found"
          fi
          
          # Additional debugging: show cloud-init logs for troubleshooting
          echo "=== Cloud-init logs (last 20 lines) ==="
          sudo tail -20 /var/log/cloud-init.log 2>/dev/null || echo "No cloud-init.log found"
          echo "=== Cloud-init output logs ==="
          sudo tail -20 /var/log/cloud-init-output.log 2>/dev/null || echo "No cloud-init-output.log found"
          echo "=== End cloud-init debugging ==="
          
          # Run comprehensive cloud-init status check
          echo "=== Running comprehensive cloud-init status check ==="
          if [ -f "/home/opc/webserver/scripts/cloud-init-check.sh" ]; then
            bash /home/opc/webserver/scripts/cloud-init-check.sh
          elif [ -f "/home/opc/web-infra/scripts/cloud-init-check.sh" ]; then
            bash /home/opc/web-infra/scripts/cloud-init-check.sh
          else
            echo "Cloud-init check script not found, using basic status"
            cloud-init status --long
          fi
          CLOUD_INIT_EOF
          
          # Step 3: Setup system configuration
          echo "Step 3: Setting up system configuration..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=15 opc@$PUBLIC_IP << 'SYSTEM_SETUP_EOF'
          set -e
          
          echo "Setting up system configuration..."
          
          # Enable user lingering and setup subuid/subgid
          sudo loginctl enable-linger opc
          if ! grep -q "^opc:" /etc/subuid; then echo "opc:100000:65536" >> /etc/subuid; fi
          if ! grep -q "^opc:" /etc/subgid; then echo "opc:100000:65536" >> /etc/subgid; fi
          
          # Setup cron job for podman cleanup
          (crontab -l 2>/dev/null; echo "0 2 * * * /usr/local/bin/podman-cleanup.sh >> /var/log/podman-cleanup.log 2>&1") | crontab -
          
          # Create cleanup script
          sudo tee /usr/local/bin/podman-cleanup.sh > /dev/null << 'CLEANUP_EOF'
          #!/bin/bash
          for user in $(getent passwd | grep -E '/home|/var/lib' | cut -d: -f1); do
            if id "$user" &>/dev/null; then
              sudo -u "$user" podman container prune -f 2>/dev/null || true
              sudo -u "$user" podman image prune -af --filter "until=24h" 2>/dev/null || true
              sudo -u "$user" podman volume prune -f 2>/dev/null || true
              sudo -u "$user" podman system prune -af 2>/dev/null || true
            fi
          done
          podman container prune -f; podman image prune -af --filter "until=24h"; podman volume prune -f; podman system prune -af
          buildah rmi --prune; journalctl --vacuum-time=7d; journalctl --vacuum-size=500M
          CLEANUP_EOF
          
          sudo chmod +x /usr/local/bin/podman-cleanup.sh
          
          echo "‚úÖ System configuration complete"
          SYSTEM_SETUP_EOF
          
          # Step 4: Setup container infrastructure
          echo "Step 4: Setting up container infrastructure..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=15 opc@$PUBLIC_IP << 'CONTAINER_SETUP_EOF'
          set -e
          
          echo "Setting up container infrastructure..."
          
          # Clone repository if not exists
          if [ ! -d "~/webserver" ]; then
            git clone https://github.com/VerilyPete/webserver.git ~/webserver
          else
            cd ~/webserver && git pull && cd ~
          fi
          
          # Ensure scripts directory exists and copy cloud-init check script
          mkdir -p ~/webserver/scripts
          if [ -f "scripts/cloud-init-check.sh" ]; then
            cp scripts/cloud-init-check.sh ~/webserver/scripts/
            chmod +x ~/webserver/scripts/cloud-init-check.sh
          fi
          
          # Create systemd service files
          mkdir -p ~/.config/systemd/user
          
          cat > ~/.config/systemd/user/webserver-pod.service << 'SERVICE_EOF'
          [Unit]
          Description=Web Infrastructure Pod
          Wants=network-online.target
          After=network-online.target
          RequiresMountsFor=%t/containers
          [Service]
          Type=simple
          Restart=on-failure
          TimeoutStopSec=70
          WorkingDirectory=%h/webserver
          Environment=PODMAN_SYSTEMD_UNIT=%n
          ExecStartPre=/usr/bin/podman pod create --name webserver-pod --publish 8081:8081 --replace
          ExecStart=/usr/bin/bash -c 'cd %h/webserver && /usr/local/bin/start-web-pod.sh'
          ExecStop=/usr/bin/podman pod stop webserver-pod
          ExecStopPost=/usr/bin/podman pod rm -f webserver-pod
          [Install]
          WantedBy=default.target
          SERVICE_EOF
          
          # Create start script
          sudo tee /usr/local/bin/start-web-pod.sh > /dev/null << 'START_EOF'
          #!/bin/bash
          set -e
          if [ -f ".env" ]; then
            set -a && source .env && set +a
          else
            echo "ERROR: .env file not found" && exit 1
          fi
          
          # Ensure the pod exists
          echo "Ensuring webserver-pod exists..."
          podman pod create --name webserver-pod --publish 8081:8081 --replace 2>/dev/null || true
          
          # Stop and remove existing containers to avoid conflicts
          podman stop web-pod web tailscale cloudflared 2>/dev/null || true
          podman rm web-pod web tailscale cloudflared 2>/dev/null || true
          
          # Start containers
          podman run -d --name web-pod --pod webserver-pod --restart unless-stopped k8s.gcr.io/pause:3.9
          podman run -d --name web --pod webserver-pod --restart unless-stopped ghcr.io/verilypete/webserver:latest
          if [ ! -z "$TAILSCALE_AUTH_KEY" ] && [ "$TAILSCALE_AUTH_KEY" != "tskey-auth-xxxxxxxxx" ]; then
            podman run -d --name tailscale --pod webserver-pod --privileged --restart unless-stopped \
              --volume tailscale-data:/var/lib/tailscale:z --volume /dev/net/tun:/dev/net/tun --cap-add NET_ADMIN \
              --env TS_AUTHKEY="$TAILSCALE_AUTH_KEY" --env TS_HOSTNAME="$HOSTNAME" --env TS_STATE_DIR=/var/lib/tailscale \
              tailscale/tailscale:latest
          fi
          if [ ! -z "$CLOUDFLARE_TUNNEL_TOKEN" ] && [ "$CLOUDFLARE_TUNNEL_TOKEN" != "your-tunnel-token-here" ]; then
            podman run -d --name cloudflared --pod webserver-pod --restart unless-stopped \
              --env TUNNEL_TOKEN="$CLOUDFLARE_TUNNEL_TOKEN" cloudflare/cloudflared:latest tunnel --no-autoupdate run
          fi
          
          echo "‚úÖ Web infrastructure started successfully"
          START_EOF
          
          sudo chmod +x /usr/local/bin/start-web-pod.sh
          
          # Setup user systemd environment
          export XDG_RUNTIME_DIR="/run/user/$(id -u)"
          sudo systemctl start user@$(id -u).service
          sleep 2
          systemctl --user daemon-reload
          systemctl --user enable webserver-pod.service
          
          echo "‚úÖ Container infrastructure setup complete"
          CONTAINER_SETUP_EOF
          
          # Step 5: Deploy web infrastructure using deploy.sh script
          echo "Step 5: Deploying web infrastructure using deploy.sh script..."
          ssh -o StrictHostKeyChecking=no -o ConnectTimeout=15 opc@$PUBLIC_IP << 'WEB_INFRA_EOF'
          set -e
          
          echo "Setting up environment variables..."
          cd ~/webserver
          
          # Create .env file directly
          cat > .env << 'ENV_EOF'
          HOSTNAME=${{ steps.hostname.outputs.hostname }}
          TAILSCALE_AUTH_KEY=${{ secrets.TAILSCALE_AUTH_KEY }}
          CLOUDFLARE_TUNNEL_TOKEN=${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
          FORMSPREE_ENDPOINT=${{ secrets.FORMSPREE_ENDPOINT }}
          APP_PORT=8081
          APP_ENV=production
          ENV_EOF
          
          chmod 600 .env
          
          echo "Running deploy.sh script..."
          ./scripts/deploy.sh || {
            echo "‚ùå Deploy script failed"
            exit 1
          }
          
          echo "‚úÖ Web infrastructure deployed successfully!"
          WEB_INFRA_EOF

      - name: Verify deployment
        run: |
          echo "‚úÖ Deployment completed successfully!"
          echo ""
          if [ "${{ github.event.inputs.deploy_type }}" = "fresh_deploy" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "üÜï New instance created:"
            echo "   Instance ID: ${{ steps.parse_instance_id.outputs.instance_id }}"
            echo "   Public IP: ${{ steps.get_new_ip.outputs.output }}"
            echo "   Hostname: ${{ steps.hostname.outputs.hostname }}"
          else
            echo "üîÑ Existing instance updated:"
            echo "   Instance ID: ${{ steps.check_instance.outputs.instance_id }}"
            echo "   Public IP: ${{ steps.get_instance_ip.outputs.output }}"
          fi
          echo ""
          echo "üîç Check your Tailscale admin console for the device"
          echo "üåê Web server will be accessible at http://[tailscale-ip]:8081"
          echo "üìä Monitor with: ssh opc@[instance-ip] 'podman pod ps && podman ps'"

  cleanup-old-instances:
    runs-on: ubuntu-latest
    timeout-minutes: 8  # Increased slightly
    needs: deploy
    if: github.event.inputs.deploy_type == 'fresh_deploy'
    
    env:
      OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
      OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
      OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
      OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
      OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}
    
    steps:
      - name: List old instances for manual cleanup
        uses: oracle-actions/run-oci-cli-command@v1.3.2
        timeout-minutes: 3
        with:
          command: 'compute instance list --compartment-id "${{ secrets.OCI_COMPARTMENT_ID }}" --lifecycle-state RUNNING'
          query: 'data[?contains(\"display-name\", `web-server`)].{Name:\"display-name\", ID:id, Created:\"time-created\"}'
          silent: false

      - name: Cleanup instructions
        run: |
          echo ""
          echo "üßπ Old instances listed above may need cleanup"
          echo "üí° To terminate old instances:"
          echo "   Use the OCI Console or run:"
          echo "   oci compute instance terminate --instance-id <INSTANCE_ID> --force"